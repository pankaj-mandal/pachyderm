# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: api/pps/pps.proto
# plugin: python-betterproto
from dataclasses import dataclass
from datetime import (
    datetime,
    timedelta,
)
from typing import (
    TYPE_CHECKING,
    AsyncIterator,
    Dict,
    Iterator,
    List,
    Optional,
)

import betterproto
import betterproto.lib.google.protobuf as betterproto_lib_google_protobuf
import grpc
from betterproto.grpc.grpcio_server import ServicerBase

from .. import (
    pfs_v2 as _pfs_v2__,
    taskapi as _taskapi__,
)


if TYPE_CHECKING:
    import grpc
    from betterproto.grpc.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


class JobState(betterproto.Enum):
    JOB_STATE_UNKNOWN = 0
    JOB_CREATED = 1
    JOB_STARTING = 2
    JOB_RUNNING = 3
    JOB_FAILURE = 4
    JOB_SUCCESS = 5
    JOB_KILLED = 6
    JOB_EGRESSING = 7
    JOB_FINISHING = 8
    JOB_UNRUNNABLE = 9


class DatumState(betterproto.Enum):
    UNKNOWN = 0
    FAILED = 1
    SUCCESS = 2
    SKIPPED = 3
    STARTING = 4
    RECOVERED = 5


class WorkerState(betterproto.Enum):
    WORKER_STATE_UNKNOWN = 0
    POD_RUNNING = 1
    POD_SUCCESS = 2
    POD_FAILED = 3


class PipelineState(betterproto.Enum):
    PIPELINE_STATE_UNKNOWN = 0
    PIPELINE_STARTING = 1
    """
    There is a PipelineInfo + spec commit, but no RC This happens when a
    pipeline has been created but not yet picked up by a PPS server.
    """

    PIPELINE_RUNNING = 2
    """
    A pipeline has a spec commit and a service + RC This is the normal state of
    a pipeline.
    """

    PIPELINE_RESTARTING = 3
    """
    Equivalent to STARTING (there is a PipelineInfo + commit, but no RC) After
    some error caused runPipeline to exit, but before the pipeline is re-run.
    This is when the exponential backoff is in effect.
    """

    PIPELINE_FAILURE = 4
    """
    The pipeline has encountered unrecoverable errors and is no longer being
    retried. It won't leave this state until the pipeline is updated.
    """

    PIPELINE_PAUSED = 5
    """
    The pipeline has been explicitly paused by the user (the pipeline spec's
    Stopped field should be true if the pipeline is in this state)
    """

    PIPELINE_STANDBY = 6
    """
    The pipeline is fully functional, but there are no commits to process.
    """

    PIPELINE_CRASHING = 7
    """
    The pipeline's workers are crashing, or failing to come up, this may
    resolve itself, the pipeline may make progress while in this state if the
    problem is only being experienced by some workers.
    """


class PipelineInfoPipelineType(betterproto.Enum):
    """
    The pipeline type is stored here so that we can internally know the type of
    the pipeline without loading the spec from PFS.
    """

    PIPELINT_TYPE_UNKNOWN = 0
    PIPELINE_TYPE_TRANSFORM = 1
    PIPELINE_TYPE_SPOUT = 2
    PIPELINE_TYPE_SERVICE = 3


@dataclass(eq=False, repr=False)
class SecretMount(betterproto.Message):
    name: str = betterproto.string_field(1)
    """Name must be the name of the secret in kubernetes."""

    key: str = betterproto.string_field(2)
    """
    Key of the secret to load into env_var, this field only has meaning if
    EnvVar != "".
    """

    mount_path: str = betterproto.string_field(3)
    env_var: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class Transform(betterproto.Message):
    image: str = betterproto.string_field(1)
    cmd: List[str] = betterproto.string_field(2)
    err_cmd: List[str] = betterproto.string_field(3)
    env: Dict[str, str] = betterproto.map_field(
        4, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )
    secrets: List["SecretMount"] = betterproto.message_field(5)
    image_pull_secrets: List[str] = betterproto.string_field(6)
    stdin: List[str] = betterproto.string_field(7)
    err_stdin: List[str] = betterproto.string_field(8)
    accept_return_code: List[int] = betterproto.int64_field(9)
    debug: bool = betterproto.bool_field(10)
    user: str = betterproto.string_field(11)
    working_dir: str = betterproto.string_field(12)
    dockerfile: str = betterproto.string_field(13)


@dataclass(eq=False, repr=False)
class TfJob(betterproto.Message):
    tf_job: str = betterproto.string_field(1)
    """
    tf_job  is a serialized Kubeflow TFJob spec. Pachyderm sends this directly
    to a kubernetes cluster on which kubeflow has been installed, instead of
    creating a pipeline ReplicationController as it normally would.
    """


@dataclass(eq=False, repr=False)
class Egress(betterproto.Message):
    url: str = betterproto.string_field(1)
    object_storage: "_pfs_v2__.ObjectStorageEgress" = betterproto.message_field(
        2, group="target"
    )
    sql_database: "_pfs_v2__.SqlDatabaseEgress" = betterproto.message_field(
        3, group="target"
    )


@dataclass(eq=False, repr=False)
class Job(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)
    id: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class Metadata(betterproto.Message):
    annotations: Dict[str, str] = betterproto.map_field(
        1, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )
    labels: Dict[str, str] = betterproto.map_field(
        2, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )


@dataclass(eq=False, repr=False)
class Service(betterproto.Message):
    internal_port: int = betterproto.int32_field(1)
    external_port: int = betterproto.int32_field(2)
    ip: str = betterproto.string_field(3)
    type: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class Spout(betterproto.Message):
    service: "Service" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class PfsInput(betterproto.Message):
    name: str = betterproto.string_field(1)
    repo: str = betterproto.string_field(2)
    repo_type: str = betterproto.string_field(13)
    branch: str = betterproto.string_field(3)
    commit: str = betterproto.string_field(4)
    glob: str = betterproto.string_field(5)
    join_on: str = betterproto.string_field(6)
    outer_join: bool = betterproto.bool_field(7)
    group_by: str = betterproto.string_field(8)
    lazy: bool = betterproto.bool_field(9)
    empty_files: bool = betterproto.bool_field(10)
    """
    EmptyFiles, if true, will cause files from this PFS input to be presented
    as empty files. This is useful in shuffle pipelines where you want to read
    the names of files and reorganize them using symlinks.
    """

    s3: bool = betterproto.bool_field(11)
    """
    S3, if true, will cause the worker to NOT download or link files from this
    input into the /pfs_v2 directory. Instead, an instance of our S3 gateway
    service will run on each of the sidecars, and data can be retrieved from
    this input by querying http://<pipeline>-s3.<namespace>/<job
    id>.<input>/my/file
    """

    trigger: "_pfs_v2__.Trigger" = betterproto.message_field(12)
    """
    Trigger defines when this input is processed by the pipeline, if it's nil
    the input is processed anytime something is committed to the input branch.
    """


@dataclass(eq=False, repr=False)
class CronInput(betterproto.Message):
    name: str = betterproto.string_field(1)
    repo: str = betterproto.string_field(2)
    commit: str = betterproto.string_field(3)
    spec: str = betterproto.string_field(4)
    overwrite: bool = betterproto.bool_field(5)
    """
    Overwrite, if true, will expose a single datum that gets overwritten each
    tick. If false, it will create a new datum for each tick.
    """

    start: datetime = betterproto.message_field(6)


@dataclass(eq=False, repr=False)
class Input(betterproto.Message):
    pfs: "PfsInput" = betterproto.message_field(1)
    join: List["Input"] = betterproto.message_field(2)
    group: List["Input"] = betterproto.message_field(3)
    cross: List["Input"] = betterproto.message_field(4)
    union: List["Input"] = betterproto.message_field(5)
    cron: "CronInput" = betterproto.message_field(6)


@dataclass(eq=False, repr=False)
class JobInput(betterproto.Message):
    name: str = betterproto.string_field(1)
    commit: "_pfs_v2__.Commit" = betterproto.message_field(2)
    glob: str = betterproto.string_field(3)
    lazy: bool = betterproto.bool_field(4)


@dataclass(eq=False, repr=False)
class ParallelismSpec(betterproto.Message):
    constant: int = betterproto.uint64_field(1)
    """
    Starts the pipeline/job with a 'constant' workers, unless 'constant' is
    zero. If 'constant' is zero (which is the zero value of ParallelismSpec),
    then Pachyderm will choose the number of workers that is started,
    (currently it chooses the number of workers in the cluster)
    """


@dataclass(eq=False, repr=False)
class InputFile(betterproto.Message):
    path: str = betterproto.string_field(1)
    """This file's absolute path within its pfs repo."""

    hash: bytes = betterproto.bytes_field(2)
    """This file's hash"""


@dataclass(eq=False, repr=False)
class Datum(betterproto.Message):
    job: "Job" = betterproto.message_field(1)
    """ID is the hash computed from all the files"""

    id: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class DatumInfo(betterproto.Message):
    datum: "Datum" = betterproto.message_field(1)
    state: "DatumState" = betterproto.enum_field(2)
    stats: "ProcessStats" = betterproto.message_field(3)
    pfs_state: "_pfs_v2__.File" = betterproto.message_field(4)
    data: List["_pfs_v2__.FileInfo"] = betterproto.message_field(5)
    image_id: str = betterproto.string_field(6)


@dataclass(eq=False, repr=False)
class Aggregate(betterproto.Message):
    count: int = betterproto.int64_field(1)
    mean: float = betterproto.double_field(2)
    stddev: float = betterproto.double_field(3)
    fifth_percentile: float = betterproto.double_field(4)
    ninety_fifth_percentile: float = betterproto.double_field(5)


@dataclass(eq=False, repr=False)
class ProcessStats(betterproto.Message):
    download_time: timedelta = betterproto.message_field(1)
    process_time: timedelta = betterproto.message_field(2)
    upload_time: timedelta = betterproto.message_field(3)
    download_bytes: int = betterproto.int64_field(4)
    upload_bytes: int = betterproto.int64_field(5)


@dataclass(eq=False, repr=False)
class AggregateProcessStats(betterproto.Message):
    download_time: "Aggregate" = betterproto.message_field(1)
    process_time: "Aggregate" = betterproto.message_field(2)
    upload_time: "Aggregate" = betterproto.message_field(3)
    download_bytes: "Aggregate" = betterproto.message_field(4)
    upload_bytes: "Aggregate" = betterproto.message_field(5)


@dataclass(eq=False, repr=False)
class WorkerStatus(betterproto.Message):
    worker_id: str = betterproto.string_field(1)
    job_id: str = betterproto.string_field(2)
    datum_status: "DatumStatus" = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class DatumStatus(betterproto.Message):
    started: datetime = betterproto.message_field(1)
    """Started is the time processing on the current datum began."""

    data: List["InputFile"] = betterproto.message_field(2)


@dataclass(eq=False, repr=False)
class ResourceSpec(betterproto.Message):
    """
    ResourceSpec describes the amount of resources that pipeline pods should
    request from kubernetes, for scheduling.
    """

    cpu: float = betterproto.float_field(1)
    """
    The number of CPUs each worker needs (partial values are allowed, and
    encouraged)
    """

    memory: str = betterproto.string_field(2)
    """
    The amount of memory each worker needs (in bytes, with allowed SI suffixes
    (M, K, G, Mi, Ki, Gi, etc).
    """

    gpu: "GpuSpec" = betterproto.message_field(3)
    """The spec for GPU resources."""

    disk: str = betterproto.string_field(4)
    """
    The amount of ephemeral storage each worker needs (in bytes, with allowed
    SI suffixes (M, K, G, Mi, Ki, Gi, etc).
    """


@dataclass(eq=False, repr=False)
class GpuSpec(betterproto.Message):
    type: str = betterproto.string_field(1)
    """The type of GPU (nvidia.com/gpu or amd.com/gpu for example)."""

    number: int = betterproto.int64_field(2)
    """The number of GPUs to request."""


@dataclass(eq=False, repr=False)
class JobSetInfo(betterproto.Message):
    job_set: "JobSet" = betterproto.message_field(1)
    jobs: List["JobInfo"] = betterproto.message_field(2)


@dataclass(eq=False, repr=False)
class JobInfo(betterproto.Message):
    """
    JobInfo is the data stored in the database regarding a given job.  The
    'details' field contains more information about the job which is expensive
    to fetch, requiring querying workers or loading the pipeline spec from
    object storage.
    """

    job: "Job" = betterproto.message_field(1)
    pipeline_version: int = betterproto.uint64_field(2)
    output_commit: "_pfs_v2__.Commit" = betterproto.message_field(3)
    restart: int = betterproto.uint64_field(4)
    """Job restart count (e.g. due to datum failure)"""

    data_processed: int = betterproto.int64_field(5)
    """Counts of how many times we processed or skipped a datum"""

    data_skipped: int = betterproto.int64_field(6)
    data_total: int = betterproto.int64_field(7)
    data_failed: int = betterproto.int64_field(8)
    data_recovered: int = betterproto.int64_field(9)
    stats: "ProcessStats" = betterproto.message_field(10)
    """Download/process/upload time and download/upload bytes"""

    state: "JobState" = betterproto.enum_field(11)
    reason: str = betterproto.string_field(12)
    created: datetime = betterproto.message_field(13)
    started: datetime = betterproto.message_field(14)
    finished: datetime = betterproto.message_field(15)
    details: "JobInfoDetails" = betterproto.message_field(16)


@dataclass(eq=False, repr=False)
class JobInfoDetails(betterproto.Message):
    transform: "Transform" = betterproto.message_field(1)
    parallelism_spec: "ParallelismSpec" = betterproto.message_field(2)
    egress: "Egress" = betterproto.message_field(3)
    service: "Service" = betterproto.message_field(4)
    spout: "Spout" = betterproto.message_field(5)
    worker_status: List["WorkerStatus"] = betterproto.message_field(6)
    resource_requests: "ResourceSpec" = betterproto.message_field(7)
    resource_limits: "ResourceSpec" = betterproto.message_field(8)
    sidecar_resource_limits: "ResourceSpec" = betterproto.message_field(9)
    input: "Input" = betterproto.message_field(10)
    salt: str = betterproto.string_field(11)
    datum_set_spec: "DatumSetSpec" = betterproto.message_field(12)
    datum_timeout: timedelta = betterproto.message_field(13)
    job_timeout: timedelta = betterproto.message_field(14)
    datum_tries: int = betterproto.int64_field(15)
    scheduling_spec: "SchedulingSpec" = betterproto.message_field(16)
    pod_spec: str = betterproto.string_field(17)
    pod_patch: str = betterproto.string_field(18)


@dataclass(eq=False, repr=False)
class Worker(betterproto.Message):
    name: str = betterproto.string_field(1)
    state: "WorkerState" = betterproto.enum_field(2)


@dataclass(eq=False, repr=False)
class Pipeline(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class PipelineInfo(betterproto.Message):
    """
    PipelineInfo is proto for each pipeline that Pachd stores in the database.
    It tracks the state of the pipeline, and points to its metadata in PFS
    (and, by pointing to a PFS commit, de facto tracks the pipeline's version).
    Any information about the pipeline _not_ stored in the database is in the
    Details object, which requires fetching the spec from PFS or other
    potentially expensive operations.
    """

    pipeline: "Pipeline" = betterproto.message_field(1)
    version: int = betterproto.uint64_field(2)
    spec_commit: "_pfs_v2__.Commit" = betterproto.message_field(3)
    stopped: bool = betterproto.bool_field(4)
    state: "PipelineState" = betterproto.enum_field(5)
    """state indicates the current state of the pipeline"""

    reason: str = betterproto.string_field(6)
    """reason includes any error messages associated with a failed pipeline"""

    last_job_state: "JobState" = betterproto.enum_field(8)
    """last_job_state indicates the state of the most recently created job"""

    parallelism: int = betterproto.uint64_field(9)
    """
    parallelism tracks the literal number of workers that this pipeline should
    run.
    """

    type: "PipelineInfoPipelineType" = betterproto.enum_field(10)
    auth_token: str = betterproto.string_field(11)
    details: "PipelineInfoDetails" = betterproto.message_field(12)


@dataclass(eq=False, repr=False)
class PipelineInfoDetails(betterproto.Message):
    transform: "Transform" = betterproto.message_field(1)
    tf_job: "TfJob" = betterproto.message_field(2)
    """
    tf_job encodes a Kubeflow TFJob spec. Pachyderm uses this to create TFJobs
    when running in a kubernetes cluster on which kubeflow has been installed.
    Exactly one of 'tf_job' and 'transform' should be set
    """

    parallelism_spec: "ParallelismSpec" = betterproto.message_field(3)
    egress: "Egress" = betterproto.message_field(4)
    created_at: datetime = betterproto.message_field(5)
    recent_error: str = betterproto.string_field(6)
    workers_requested: int = betterproto.int64_field(7)
    workers_available: int = betterproto.int64_field(8)
    output_branch: str = betterproto.string_field(9)
    resource_requests: "ResourceSpec" = betterproto.message_field(10)
    resource_limits: "ResourceSpec" = betterproto.message_field(11)
    sidecar_resource_limits: "ResourceSpec" = betterproto.message_field(12)
    input: "Input" = betterproto.message_field(13)
    description: str = betterproto.string_field(14)
    salt: str = betterproto.string_field(16)
    reason: str = betterproto.string_field(17)
    service: "Service" = betterproto.message_field(19)
    spout: "Spout" = betterproto.message_field(20)
    datum_set_spec: "DatumSetSpec" = betterproto.message_field(21)
    datum_timeout: timedelta = betterproto.message_field(22)
    job_timeout: timedelta = betterproto.message_field(23)
    datum_tries: int = betterproto.int64_field(24)
    scheduling_spec: "SchedulingSpec" = betterproto.message_field(25)
    pod_spec: str = betterproto.string_field(26)
    pod_patch: str = betterproto.string_field(27)
    s3_out: bool = betterproto.bool_field(28)
    metadata: "Metadata" = betterproto.message_field(29)
    reprocess_spec: str = betterproto.string_field(30)
    unclaimed_tasks: int = betterproto.int64_field(31)
    worker_rc: str = betterproto.string_field(32)
    autoscaling: bool = betterproto.bool_field(33)


@dataclass(eq=False, repr=False)
class PipelineInfos(betterproto.Message):
    pipeline_info: List["PipelineInfo"] = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class JobSet(betterproto.Message):
    id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class InspectJobSetRequest(betterproto.Message):
    job_set: "JobSet" = betterproto.message_field(1)
    wait: bool = betterproto.bool_field(2)
    details: bool = betterproto.bool_field(3)


@dataclass(eq=False, repr=False)
class ListJobSetRequest(betterproto.Message):
    details: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class InspectJobRequest(betterproto.Message):
    job: "Job" = betterproto.message_field(1)
    """Callers should set either Job or OutputCommit, not both."""

    wait: bool = betterproto.bool_field(2)
    details: bool = betterproto.bool_field(3)


@dataclass(eq=False, repr=False)
class ListJobRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)
    input_commit: List["_pfs_v2__.Commit"] = betterproto.message_field(2)
    history: int = betterproto.int64_field(4)
    """
    History indicates return jobs from historical versions of pipelines
    semantics are: 0: Return jobs from the current version of the pipeline or
    pipelines. 1: Return the above and jobs from the next most recent version
    2: etc.-1: Return jobs from all historical versions.
    """

    details: bool = betterproto.bool_field(5)
    """
    Details indicates whether the result should include all pipeline details in
    each JobInfo, or limited information including name and status, but
    excluding information in the pipeline spec. Leaving this "false" can make
    the call significantly faster in clusters with a large number of pipelines
    and jobs. Note that if 'input_commit' is set, this field is coerced to
    "true"
    """

    jq_filter: str = betterproto.string_field(6)
    """A jq program string for additional result filtering"""


@dataclass(eq=False, repr=False)
class SubscribeJobRequest(betterproto.Message):
    """Streams open jobs until canceled"""

    pipeline: "Pipeline" = betterproto.message_field(1)
    details: bool = betterproto.bool_field(2)


@dataclass(eq=False, repr=False)
class DeleteJobRequest(betterproto.Message):
    job: "Job" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class StopJobRequest(betterproto.Message):
    job: "Job" = betterproto.message_field(1)
    reason: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class UpdateJobStateRequest(betterproto.Message):
    job: "Job" = betterproto.message_field(1)
    state: "JobState" = betterproto.enum_field(2)
    reason: str = betterproto.string_field(3)
    restart: int = betterproto.uint64_field(5)
    data_processed: int = betterproto.int64_field(6)
    data_skipped: int = betterproto.int64_field(7)
    data_failed: int = betterproto.int64_field(8)
    data_recovered: int = betterproto.int64_field(9)
    data_total: int = betterproto.int64_field(10)
    stats: "ProcessStats" = betterproto.message_field(11)


@dataclass(eq=False, repr=False)
class GetLogsRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)
    """
    The pipeline from which we want to get logs (required if the job in 'job'
    was created as part of a pipeline. To get logs from a non-orphan job
    without the pipeline that created it, you need to use ElasticSearch).
    """

    job: "Job" = betterproto.message_field(2)
    """The job from which we want to get logs."""

    data_filters: List[str] = betterproto.string_field(3)
    """
    Names of input files from which we want processing logs. This may contain
    multiple files, to query pipelines that contain multiple inputs. Each
    filter may be an absolute path of a file within a pps repo, or it may be a
    hash for that file (to search for files at specific versions)
    """

    datum: "Datum" = betterproto.message_field(4)
    master: bool = betterproto.bool_field(5)
    """If true get logs from the master process"""

    follow: bool = betterproto.bool_field(6)
    """Continue to follow new logs as they become available."""

    tail: int = betterproto.int64_field(7)
    """
    If nonzero, the number of lines from the end of the logs to return.  Note:
    tail applies per container, so you will get tail * <number of pods> total
    lines back.
    """

    use_loki_backend: bool = betterproto.bool_field(8)
    """
    UseLokiBackend causes the logs request to go through the loki backend
    rather than through kubernetes. This behavior can also be achieved by
    setting the LOKI_LOGGING feature flag.
    """

    since: timedelta = betterproto.message_field(9)
    """
    Since specifies how far in the past to return logs from. It defaults to 24
    hours.
    """


@dataclass(eq=False, repr=False)
class LogMessage(betterproto.Message):
    """
    LogMessage is a log line from a PPS worker, annotated with metadata
    indicating when and why the line was logged.
    """

    pipeline_name: str = betterproto.string_field(1)
    """
    The job and pipeline for which a PFS file is being processed (if the job is
    an orphan job, pipeline name and ID will be unset)
    """

    job_id: str = betterproto.string_field(2)
    worker_id: str = betterproto.string_field(3)
    datum_id: str = betterproto.string_field(4)
    master: bool = betterproto.bool_field(5)
    data: List["InputFile"] = betterproto.message_field(6)
    """The PFS files being processed (one per pipeline/job input)"""

    user: bool = betterproto.bool_field(7)
    """User is true if log message comes from the users code."""

    ts: datetime = betterproto.message_field(8)
    """The message logged, and the time at which it was logged"""

    message: str = betterproto.string_field(9)


@dataclass(eq=False, repr=False)
class RestartDatumRequest(betterproto.Message):
    job: "Job" = betterproto.message_field(1)
    data_filters: List[str] = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class InspectDatumRequest(betterproto.Message):
    datum: "Datum" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class ListDatumRequest(betterproto.Message):
    job: "Job" = betterproto.message_field(1)
    """
    Job and Input are two different ways to specify the datums you want. Only
    one can be set. Job is the job to list datums from.
    """

    input: "Input" = betterproto.message_field(2)
    """
    Input is the input to list datums from. The datums listed are the ones that
    would be run if a pipeline was created with the provided input.
    """

    filter: "ListDatumRequestFilter" = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class ListDatumRequestFilter(betterproto.Message):
    """
    Filter restricts returned DatumInfo messages to those which match all of
    the filtered attributes.
    """

    state: List["DatumState"] = betterproto.enum_field(1)


@dataclass(eq=False, repr=False)
class DatumSetSpec(betterproto.Message):
    """
    DatumSetSpec specifies how a pipeline should split its datums into datum
    sets.
    """

    number: int = betterproto.int64_field(1)
    """
    number, if nonzero, specifies that each datum set should contain `number`
    datums. Datum sets may contain fewer if the total number of datums don't
    divide evenly.
    """

    size_bytes: int = betterproto.int64_field(2)
    """
    size_bytes, if nonzero, specifies a target size for each datum set. Datum
    sets may be larger or smaller than size_bytes, but will usually be pretty
    close to size_bytes in size.
    """

    per_worker: int = betterproto.int64_field(3)
    """
    per_worker, if nonzero, specifies how many datum sets should be created for
    each worker. It can't be set with number or size_bytes.
    """


@dataclass(eq=False, repr=False)
class SchedulingSpec(betterproto.Message):
    node_selector: Dict[str, str] = betterproto.map_field(
        1, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )
    priority_class_name: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class CreatePipelineRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)
    tf_job: "TfJob" = betterproto.message_field(2)
    """
    tf_job encodes a Kubeflow TFJob spec. Pachyderm uses this to create TFJobs
    when running in a kubernetes cluster on which kubeflow has been installed.
    Exactly one of 'tf_job' and 'transform' should be set
    """

    transform: "Transform" = betterproto.message_field(3)
    parallelism_spec: "ParallelismSpec" = betterproto.message_field(4)
    egress: "Egress" = betterproto.message_field(5)
    update: bool = betterproto.bool_field(6)
    output_branch: str = betterproto.string_field(7)
    s3_out: bool = betterproto.bool_field(8)
    """
    s3_out, if set, requires a pipeline's user to write to its output repo via
    Pachyderm's s3 gateway (if set, workers will serve Pachyderm's s3 gateway
    API at http://<pipeline>-s3.<namespace>/<job id>.out/my/file). In this mode
    /pfs_v2/out won't be walked or uploaded, and the s3 gateway service in the
    workers will allow writes to the job's output commit
    """

    resource_requests: "ResourceSpec" = betterproto.message_field(9)
    resource_limits: "ResourceSpec" = betterproto.message_field(10)
    sidecar_resource_limits: "ResourceSpec" = betterproto.message_field(11)
    input: "Input" = betterproto.message_field(12)
    description: str = betterproto.string_field(13)
    reprocess: bool = betterproto.bool_field(15)
    """
    Reprocess forces the pipeline to reprocess all datums. It only has meaning
    if Update is true
    """

    service: "Service" = betterproto.message_field(17)
    spout: "Spout" = betterproto.message_field(18)
    datum_set_spec: "DatumSetSpec" = betterproto.message_field(19)
    datum_timeout: timedelta = betterproto.message_field(20)
    job_timeout: timedelta = betterproto.message_field(21)
    salt: str = betterproto.string_field(22)
    datum_tries: int = betterproto.int64_field(23)
    scheduling_spec: "SchedulingSpec" = betterproto.message_field(24)
    pod_spec: str = betterproto.string_field(25)
    pod_patch: str = betterproto.string_field(26)
    spec_commit: "_pfs_v2__.Commit" = betterproto.message_field(27)
    metadata: "Metadata" = betterproto.message_field(28)
    reprocess_spec: str = betterproto.string_field(29)
    autoscaling: bool = betterproto.bool_field(30)


@dataclass(eq=False, repr=False)
class InspectPipelineRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)
    details: bool = betterproto.bool_field(2)
    """
    When true, return PipelineInfos with the details field, which requires
    loading the pipeline spec from PFS.
    """


@dataclass(eq=False, repr=False)
class ListPipelineRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)
    """
    If non-nil, only return info about a single pipeline, this is redundant
    with InspectPipeline unless history is non-zero.
    """

    history: int = betterproto.int64_field(2)
    """
    History indicates how many historical versions you want returned. Its
    semantics are: 0: Return the current version of the pipeline or pipelines.
    1: Return the above and the next most recent version 2: etc.-1: Return all
    historical versions.
    """

    details: bool = betterproto.bool_field(3)
    """
    When true, return PipelineInfos with the details field, which requires
    loading the pipeline spec from PFS.
    """

    jq_filter: str = betterproto.string_field(4)
    """A jq program string for additional result filtering"""


@dataclass(eq=False, repr=False)
class DeletePipelineRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)
    all: bool = betterproto.bool_field(2)
    force: bool = betterproto.bool_field(3)
    keep_repo: bool = betterproto.bool_field(4)


@dataclass(eq=False, repr=False)
class StartPipelineRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class StopPipelineRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class RunPipelineRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)
    provenance: List["_pfs_v2__.Commit"] = betterproto.message_field(2)
    job_id: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class RunCronRequest(betterproto.Message):
    pipeline: "Pipeline" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class CreateSecretRequest(betterproto.Message):
    file: bytes = betterproto.bytes_field(1)


@dataclass(eq=False, repr=False)
class DeleteSecretRequest(betterproto.Message):
    secret: "Secret" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class InspectSecretRequest(betterproto.Message):
    secret: "Secret" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class Secret(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class SecretInfo(betterproto.Message):
    secret: "Secret" = betterproto.message_field(1)
    type: str = betterproto.string_field(2)
    creation_timestamp: datetime = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class SecretInfos(betterproto.Message):
    secret_info: List["SecretInfo"] = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class ActivateAuthRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class ActivateAuthResponse(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class RunLoadTestRequest(betterproto.Message):
    dag_spec: str = betterproto.string_field(1)
    load_spec: str = betterproto.string_field(2)
    seed: int = betterproto.int64_field(3)
    parallelism: int = betterproto.int64_field(4)
    pod_patch: str = betterproto.string_field(5)


@dataclass(eq=False, repr=False)
class RunLoadTestResponse(betterproto.Message):
    error: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class RenderTemplateRequest(betterproto.Message):
    template: str = betterproto.string_field(1)
    args: Dict[str, str] = betterproto.map_field(
        2, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )


@dataclass(eq=False, repr=False)
class RenderTemplateResponse(betterproto.Message):
    json: str = betterproto.string_field(1)
    specs: List["CreatePipelineRequest"] = betterproto.message_field(2)


class ApiStub:
    def __init__(self, channel: "grpc.Channel"):
        self.__rpc_inspect_job = channel.unary_unary(
            "/pps_v2.API/InspectJob",
            request_serializer=InspectJobRequest.SerializeToString,
            response_deserializer=JobInfo.FromString,
        )
        self.__rpc_inspect_job_set = channel.unary_stream(
            "/pps_v2.API/InspectJobSet",
            request_serializer=InspectJobSetRequest.SerializeToString,
            response_deserializer=JobInfo.FromString,
        )
        self.__rpc_list_job = channel.unary_stream(
            "/pps_v2.API/ListJob",
            request_serializer=ListJobRequest.SerializeToString,
            response_deserializer=JobInfo.FromString,
        )
        self.__rpc_list_job_set = channel.unary_stream(
            "/pps_v2.API/ListJobSet",
            request_serializer=ListJobSetRequest.SerializeToString,
            response_deserializer=JobSetInfo.FromString,
        )
        self.__rpc_subscribe_job = channel.unary_stream(
            "/pps_v2.API/SubscribeJob",
            request_serializer=SubscribeJobRequest.SerializeToString,
            response_deserializer=JobInfo.FromString,
        )
        self.__rpc_delete_job = channel.unary_unary(
            "/pps_v2.API/DeleteJob",
            request_serializer=DeleteJobRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_stop_job = channel.unary_unary(
            "/pps_v2.API/StopJob",
            request_serializer=StopJobRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_inspect_datum = channel.unary_unary(
            "/pps_v2.API/InspectDatum",
            request_serializer=InspectDatumRequest.SerializeToString,
            response_deserializer=DatumInfo.FromString,
        )
        self.__rpc_list_datum = channel.unary_stream(
            "/pps_v2.API/ListDatum",
            request_serializer=ListDatumRequest.SerializeToString,
            response_deserializer=DatumInfo.FromString,
        )
        self.__rpc_restart_datum = channel.unary_unary(
            "/pps_v2.API/RestartDatum",
            request_serializer=RestartDatumRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_create_pipeline = channel.unary_unary(
            "/pps_v2.API/CreatePipeline",
            request_serializer=CreatePipelineRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_inspect_pipeline = channel.unary_unary(
            "/pps_v2.API/InspectPipeline",
            request_serializer=InspectPipelineRequest.SerializeToString,
            response_deserializer=PipelineInfo.FromString,
        )
        self.__rpc_list_pipeline = channel.unary_stream(
            "/pps_v2.API/ListPipeline",
            request_serializer=ListPipelineRequest.SerializeToString,
            response_deserializer=PipelineInfo.FromString,
        )
        self.__rpc_delete_pipeline = channel.unary_unary(
            "/pps_v2.API/DeletePipeline",
            request_serializer=DeletePipelineRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_start_pipeline = channel.unary_unary(
            "/pps_v2.API/StartPipeline",
            request_serializer=StartPipelineRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_stop_pipeline = channel.unary_unary(
            "/pps_v2.API/StopPipeline",
            request_serializer=StopPipelineRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_run_pipeline = channel.unary_unary(
            "/pps_v2.API/RunPipeline",
            request_serializer=RunPipelineRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_run_cron = channel.unary_unary(
            "/pps_v2.API/RunCron",
            request_serializer=RunCronRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_create_secret = channel.unary_unary(
            "/pps_v2.API/CreateSecret",
            request_serializer=CreateSecretRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_delete_secret = channel.unary_unary(
            "/pps_v2.API/DeleteSecret",
            request_serializer=DeleteSecretRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_list_secret = channel.unary_unary(
            "/pps_v2.API/ListSecret",
            request_serializer=betterproto_lib_google_protobuf.Empty.SerializeToString,
            response_deserializer=SecretInfos.FromString,
        )
        self.__rpc_inspect_secret = channel.unary_unary(
            "/pps_v2.API/InspectSecret",
            request_serializer=InspectSecretRequest.SerializeToString,
            response_deserializer=SecretInfo.FromString,
        )
        self.__rpc_delete_all = channel.unary_unary(
            "/pps_v2.API/DeleteAll",
            request_serializer=betterproto_lib_google_protobuf.Empty.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_get_logs = channel.unary_stream(
            "/pps_v2.API/GetLogs",
            request_serializer=GetLogsRequest.SerializeToString,
            response_deserializer=LogMessage.FromString,
        )
        self.__rpc_activate_auth = channel.unary_unary(
            "/pps_v2.API/ActivateAuth",
            request_serializer=ActivateAuthRequest.SerializeToString,
            response_deserializer=ActivateAuthResponse.FromString,
        )
        self.__rpc_update_job_state = channel.unary_unary(
            "/pps_v2.API/UpdateJobState",
            request_serializer=UpdateJobStateRequest.SerializeToString,
            response_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
        )
        self.__rpc_run_load_test = channel.unary_unary(
            "/pps_v2.API/RunLoadTest",
            request_serializer=RunLoadTestRequest.SerializeToString,
            response_deserializer=RunLoadTestResponse.FromString,
        )
        self.__rpc_run_load_test_default = channel.unary_unary(
            "/pps_v2.API/RunLoadTestDefault",
            request_serializer=betterproto_lib_google_protobuf.Empty.SerializeToString,
            response_deserializer=RunLoadTestResponse.FromString,
        )
        self.__rpc_render_template = channel.unary_unary(
            "/pps_v2.API/RenderTemplate",
            request_serializer=RenderTemplateRequest.SerializeToString,
            response_deserializer=RenderTemplateResponse.FromString,
        )
        self.__rpc_list_task = channel.unary_stream(
            "/pps_v2.API/ListTask",
            request_serializer=_taskapi__.ListTaskRequest.SerializeToString,
            response_deserializer=_taskapi__.TaskInfo.FromString,
        )

    def inspect_job(self, inspect_job_request: "InspectJobRequest") -> "JobInfo":
        return self.__rpc_inspect_job(inspect_job_request)

    def inspect_job_set(
        self, inspect_job_set_request: "InspectJobSetRequest"
    ) -> Iterator["JobInfo"]:
        for response in self.__rpc_inspect_job_set(inspect_job_set_request):
            yield response

    def list_job(self, list_job_request: "ListJobRequest") -> Iterator["JobInfo"]:
        for response in self.__rpc_list_job(list_job_request):
            yield response

    def list_job_set(
        self, list_job_set_request: "ListJobSetRequest"
    ) -> Iterator["JobSetInfo"]:
        for response in self.__rpc_list_job_set(list_job_set_request):
            yield response

    def subscribe_job(
        self, subscribe_job_request: "SubscribeJobRequest"
    ) -> Iterator["JobInfo"]:
        for response in self.__rpc_subscribe_job(subscribe_job_request):
            yield response

    def delete_job(
        self, delete_job_request: "DeleteJobRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_delete_job(delete_job_request)

    def stop_job(
        self, stop_job_request: "StopJobRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_stop_job(stop_job_request)

    def inspect_datum(
        self, inspect_datum_request: "InspectDatumRequest"
    ) -> "DatumInfo":
        return self.__rpc_inspect_datum(inspect_datum_request)

    def list_datum(
        self, list_datum_request: "ListDatumRequest"
    ) -> Iterator["DatumInfo"]:
        for response in self.__rpc_list_datum(list_datum_request):
            yield response

    def restart_datum(
        self, restart_datum_request: "RestartDatumRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_restart_datum(restart_datum_request)

    def create_pipeline(
        self, create_pipeline_request: "CreatePipelineRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_create_pipeline(create_pipeline_request)

    def inspect_pipeline(
        self, inspect_pipeline_request: "InspectPipelineRequest"
    ) -> "PipelineInfo":
        return self.__rpc_inspect_pipeline(inspect_pipeline_request)

    def list_pipeline(
        self, list_pipeline_request: "ListPipelineRequest"
    ) -> Iterator["PipelineInfo"]:
        for response in self.__rpc_list_pipeline(list_pipeline_request):
            yield response

    def delete_pipeline(
        self, delete_pipeline_request: "DeletePipelineRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_delete_pipeline(delete_pipeline_request)

    def start_pipeline(
        self, start_pipeline_request: "StartPipelineRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_start_pipeline(start_pipeline_request)

    def stop_pipeline(
        self, stop_pipeline_request: "StopPipelineRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_stop_pipeline(stop_pipeline_request)

    def run_pipeline(
        self, run_pipeline_request: "RunPipelineRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_run_pipeline(run_pipeline_request)

    def run_cron(
        self, run_cron_request: "RunCronRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_run_cron(run_cron_request)

    def create_secret(
        self, create_secret_request: "CreateSecretRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_create_secret(create_secret_request)

    def delete_secret(
        self, delete_secret_request: "DeleteSecretRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_delete_secret(delete_secret_request)

    def list_secret(
        self,
        betterproto_lib_google_protobuf_empty: "betterproto_lib_google_protobuf.Empty",
    ) -> "SecretInfos":
        return self.__rpc_list_secret(betterproto_lib_google_protobuf_empty)

    def inspect_secret(
        self, inspect_secret_request: "InspectSecretRequest"
    ) -> "SecretInfo":
        return self.__rpc_inspect_secret(inspect_secret_request)

    def delete_all(
        self,
        betterproto_lib_google_protobuf_empty: "betterproto_lib_google_protobuf.Empty",
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_delete_all(betterproto_lib_google_protobuf_empty)

    def get_logs(self, get_logs_request: "GetLogsRequest") -> Iterator["LogMessage"]:
        for response in self.__rpc_get_logs(get_logs_request):
            yield response

    def activate_auth(
        self, activate_auth_request: "ActivateAuthRequest"
    ) -> "ActivateAuthResponse":
        return self.__rpc_activate_auth(activate_auth_request)

    def update_job_state(
        self, update_job_state_request: "UpdateJobStateRequest"
    ) -> "betterproto_lib_google_protobuf.Empty":
        return self.__rpc_update_job_state(update_job_state_request)

    def run_load_test(
        self, run_load_test_request: "RunLoadTestRequest"
    ) -> "RunLoadTestResponse":
        return self.__rpc_run_load_test(run_load_test_request)

    def run_load_test_default(
        self,
        betterproto_lib_google_protobuf_empty: "betterproto_lib_google_protobuf.Empty",
    ) -> "RunLoadTestResponse":
        return self.__rpc_run_load_test_default(betterproto_lib_google_protobuf_empty)

    def render_template(
        self, render_template_request: "RenderTemplateRequest"
    ) -> "RenderTemplateResponse":
        return self.__rpc_render_template(render_template_request)

    def list_task(
        self, taskapi_list_task_request: "_taskapi__.ListTaskRequest"
    ) -> Iterator["_taskapi__.TaskInfo"]:
        for response in self.__rpc_list_task(taskapi_list_task_request):
            yield response


class ApiBase(ServicerBase):
    def inspect_job(
        self, request: "InspectJobRequest", context: "grpc.ServicerContext"
    ) -> "JobInfo":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def inspect_job_set(
        self, request: "InspectJobSetRequest", context: "grpc.ServicerContext"
    ) -> Iterator["JobInfo"]:
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def list_job(
        self, request: "ListJobRequest", context: "grpc.ServicerContext"
    ) -> Iterator["JobInfo"]:
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def list_job_set(
        self, request: "ListJobSetRequest", context: "grpc.ServicerContext"
    ) -> Iterator["JobSetInfo"]:
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def subscribe_job(
        self, request: "SubscribeJobRequest", context: "grpc.ServicerContext"
    ) -> Iterator["JobInfo"]:
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def delete_job(
        self, request: "DeleteJobRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def stop_job(
        self, request: "StopJobRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def inspect_datum(
        self, request: "InspectDatumRequest", context: "grpc.ServicerContext"
    ) -> "DatumInfo":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def list_datum(
        self, request: "ListDatumRequest", context: "grpc.ServicerContext"
    ) -> Iterator["DatumInfo"]:
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def restart_datum(
        self, request: "RestartDatumRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def create_pipeline(
        self, request: "CreatePipelineRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def inspect_pipeline(
        self, request: "InspectPipelineRequest", context: "grpc.ServicerContext"
    ) -> "PipelineInfo":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def list_pipeline(
        self, request: "ListPipelineRequest", context: "grpc.ServicerContext"
    ) -> Iterator["PipelineInfo"]:
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def delete_pipeline(
        self, request: "DeletePipelineRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def start_pipeline(
        self, request: "StartPipelineRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def stop_pipeline(
        self, request: "StopPipelineRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def run_pipeline(
        self, request: "RunPipelineRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def run_cron(
        self, request: "RunCronRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def create_secret(
        self, request: "CreateSecretRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def delete_secret(
        self, request: "DeleteSecretRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def list_secret(
        self,
        request: "betterproto_lib_google_protobuf.Empty",
        context: "grpc.ServicerContext",
    ) -> "SecretInfos":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def inspect_secret(
        self, request: "InspectSecretRequest", context: "grpc.ServicerContext"
    ) -> "SecretInfo":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def delete_all(
        self,
        request: "betterproto_lib_google_protobuf.Empty",
        context: "grpc.ServicerContext",
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def get_logs(
        self, request: "GetLogsRequest", context: "grpc.ServicerContext"
    ) -> Iterator["LogMessage"]:
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def activate_auth(
        self, request: "ActivateAuthRequest", context: "grpc.ServicerContext"
    ) -> "ActivateAuthResponse":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def update_job_state(
        self, request: "UpdateJobStateRequest", context: "grpc.ServicerContext"
    ) -> "betterproto_lib_google_protobuf.Empty":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def run_load_test(
        self, request: "RunLoadTestRequest", context: "grpc.ServicerContext"
    ) -> "RunLoadTestResponse":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def run_load_test_default(
        self,
        request: "betterproto_lib_google_protobuf.Empty",
        context: "grpc.ServicerContext",
    ) -> "RunLoadTestResponse":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def render_template(
        self, request: "RenderTemplateRequest", context: "grpc.ServicerContext"
    ) -> "RenderTemplateResponse":
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    def list_task(
        self, request: "_taskapi__.ListTaskRequest", context: "grpc.ServicerContext"
    ) -> Iterator["_taskapi__.TaskInfo"]:
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details("Method not implemented!")
        raise NotImplementedError("Method not implemented!")

    __proto_path__ = "pps_v2.API"

    @property
    def __rpc_methods__(self):
        return {
            "InspectJob": grpc.unary_unary_rpc_method_handler(
                self.inspect_job,
                request_deserializer=InspectJobRequest.FromString,
                response_serializer=InspectJobRequest.SerializeToString,
            ),
            "InspectJobSet": grpc.unary_stream_rpc_method_handler(
                self.inspect_job_set,
                request_deserializer=InspectJobSetRequest.FromString,
                response_serializer=InspectJobSetRequest.SerializeToString,
            ),
            "ListJob": grpc.unary_stream_rpc_method_handler(
                self.list_job,
                request_deserializer=ListJobRequest.FromString,
                response_serializer=ListJobRequest.SerializeToString,
            ),
            "ListJobSet": grpc.unary_stream_rpc_method_handler(
                self.list_job_set,
                request_deserializer=ListJobSetRequest.FromString,
                response_serializer=ListJobSetRequest.SerializeToString,
            ),
            "SubscribeJob": grpc.unary_stream_rpc_method_handler(
                self.subscribe_job,
                request_deserializer=SubscribeJobRequest.FromString,
                response_serializer=SubscribeJobRequest.SerializeToString,
            ),
            "DeleteJob": grpc.unary_unary_rpc_method_handler(
                self.delete_job,
                request_deserializer=DeleteJobRequest.FromString,
                response_serializer=DeleteJobRequest.SerializeToString,
            ),
            "StopJob": grpc.unary_unary_rpc_method_handler(
                self.stop_job,
                request_deserializer=StopJobRequest.FromString,
                response_serializer=StopJobRequest.SerializeToString,
            ),
            "InspectDatum": grpc.unary_unary_rpc_method_handler(
                self.inspect_datum,
                request_deserializer=InspectDatumRequest.FromString,
                response_serializer=InspectDatumRequest.SerializeToString,
            ),
            "ListDatum": grpc.unary_stream_rpc_method_handler(
                self.list_datum,
                request_deserializer=ListDatumRequest.FromString,
                response_serializer=ListDatumRequest.SerializeToString,
            ),
            "RestartDatum": grpc.unary_unary_rpc_method_handler(
                self.restart_datum,
                request_deserializer=RestartDatumRequest.FromString,
                response_serializer=RestartDatumRequest.SerializeToString,
            ),
            "CreatePipeline": grpc.unary_unary_rpc_method_handler(
                self.create_pipeline,
                request_deserializer=CreatePipelineRequest.FromString,
                response_serializer=CreatePipelineRequest.SerializeToString,
            ),
            "InspectPipeline": grpc.unary_unary_rpc_method_handler(
                self.inspect_pipeline,
                request_deserializer=InspectPipelineRequest.FromString,
                response_serializer=InspectPipelineRequest.SerializeToString,
            ),
            "ListPipeline": grpc.unary_stream_rpc_method_handler(
                self.list_pipeline,
                request_deserializer=ListPipelineRequest.FromString,
                response_serializer=ListPipelineRequest.SerializeToString,
            ),
            "DeletePipeline": grpc.unary_unary_rpc_method_handler(
                self.delete_pipeline,
                request_deserializer=DeletePipelineRequest.FromString,
                response_serializer=DeletePipelineRequest.SerializeToString,
            ),
            "StartPipeline": grpc.unary_unary_rpc_method_handler(
                self.start_pipeline,
                request_deserializer=StartPipelineRequest.FromString,
                response_serializer=StartPipelineRequest.SerializeToString,
            ),
            "StopPipeline": grpc.unary_unary_rpc_method_handler(
                self.stop_pipeline,
                request_deserializer=StopPipelineRequest.FromString,
                response_serializer=StopPipelineRequest.SerializeToString,
            ),
            "RunPipeline": grpc.unary_unary_rpc_method_handler(
                self.run_pipeline,
                request_deserializer=RunPipelineRequest.FromString,
                response_serializer=RunPipelineRequest.SerializeToString,
            ),
            "RunCron": grpc.unary_unary_rpc_method_handler(
                self.run_cron,
                request_deserializer=RunCronRequest.FromString,
                response_serializer=RunCronRequest.SerializeToString,
            ),
            "CreateSecret": grpc.unary_unary_rpc_method_handler(
                self.create_secret,
                request_deserializer=CreateSecretRequest.FromString,
                response_serializer=CreateSecretRequest.SerializeToString,
            ),
            "DeleteSecret": grpc.unary_unary_rpc_method_handler(
                self.delete_secret,
                request_deserializer=DeleteSecretRequest.FromString,
                response_serializer=DeleteSecretRequest.SerializeToString,
            ),
            "ListSecret": grpc.unary_unary_rpc_method_handler(
                self.list_secret,
                request_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
                response_serializer=betterproto_lib_google_protobuf.Empty.SerializeToString,
            ),
            "InspectSecret": grpc.unary_unary_rpc_method_handler(
                self.inspect_secret,
                request_deserializer=InspectSecretRequest.FromString,
                response_serializer=InspectSecretRequest.SerializeToString,
            ),
            "DeleteAll": grpc.unary_unary_rpc_method_handler(
                self.delete_all,
                request_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
                response_serializer=betterproto_lib_google_protobuf.Empty.SerializeToString,
            ),
            "GetLogs": grpc.unary_stream_rpc_method_handler(
                self.get_logs,
                request_deserializer=GetLogsRequest.FromString,
                response_serializer=GetLogsRequest.SerializeToString,
            ),
            "ActivateAuth": grpc.unary_unary_rpc_method_handler(
                self.activate_auth,
                request_deserializer=ActivateAuthRequest.FromString,
                response_serializer=ActivateAuthRequest.SerializeToString,
            ),
            "UpdateJobState": grpc.unary_unary_rpc_method_handler(
                self.update_job_state,
                request_deserializer=UpdateJobStateRequest.FromString,
                response_serializer=UpdateJobStateRequest.SerializeToString,
            ),
            "RunLoadTest": grpc.unary_unary_rpc_method_handler(
                self.run_load_test,
                request_deserializer=RunLoadTestRequest.FromString,
                response_serializer=RunLoadTestRequest.SerializeToString,
            ),
            "RunLoadTestDefault": grpc.unary_unary_rpc_method_handler(
                self.run_load_test_default,
                request_deserializer=betterproto_lib_google_protobuf.Empty.FromString,
                response_serializer=betterproto_lib_google_protobuf.Empty.SerializeToString,
            ),
            "RenderTemplate": grpc.unary_unary_rpc_method_handler(
                self.render_template,
                request_deserializer=RenderTemplateRequest.FromString,
                response_serializer=RenderTemplateRequest.SerializeToString,
            ),
            "ListTask": grpc.unary_stream_rpc_method_handler(
                self.list_task,
                request_deserializer=_taskapi__.ListTaskRequest.FromString,
                response_serializer=_taskapi__.ListTaskRequest.SerializeToString,
            ),
        }
